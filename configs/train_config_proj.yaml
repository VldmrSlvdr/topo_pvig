# Training Configuration

# --- Paths ---
data_dir: '/mnt/f/Datasets/physionet.org/files/mimic_part_jpg'
output_base_dir: '/mnt/d/exp_results/topo_gnn' # Base directory for all results
experiment_name: 'pvig_topo_proj_conti_feasures_m_100_run2' # Unique name for this experiment run
pretrained_path_base: './pretrain' # Directory where pvig_ti_*.pth.tar etc. are stored
custom_pretrained_path: '' # Path to your custom pretrained model for projection

# --- Model Selection ---
# model_type: 'pvig_ti' # Options: 'pvig_ti', 'pvig_s', 'pvig_m', 'pvig_b'
model_type: 'pvig_m'  # Using b model to match your baseline model
# model_mode: 'dim0' # Options: 'dim0', 'dim1', 'proj'
model_mode: 'proj'
num_classes: 3 # CHF, Normal, Pneumonia
pretrained: true # Load ImageNet pretrained weights (if available for the type)
use_custom_pretrained: false # Set to true to use your custom pretrained model

# --- Training Parameters ---
device: 'auto' # 'auto', 'cuda', 'mps', 'cpu'
epochs: 100
batch_size: 32
optimizer: 'Adam'
learning_rate: 0.0002
weight_decay: 0.001 # AdamW handles decay differently, often set to 0 here
scheduler: 'StepLR' # Options: 'StepLR', 'CosineAnnealingLR', null
scheduler_params: # Example for StepLR
  step_size: 13
  gamma: 0.36

# --- Data Loading ---
num_workers: 4

# --- Logging & Checkpointing ---
log_interval_batches: 50 # Print progress every N batches (not implemented in this version, uses tqdm)
checkpoint_interval_epochs: 10 # Save checkpoint every N epochs
drop_path_rate: 0.0 # Stochastic depth drop rate

# --- Reproducibility ---
seed: 42 